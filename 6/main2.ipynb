{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Fit a Neural Radiance Field from Multi-view Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jae\\anaconda\\envs\\cvision2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import mediapy as media\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as transforms\n",
    "from diffusers import DiffusionPipeline\n",
    "from transformers import T5EncoderModel\n",
    "\n",
    "# For downloading web images\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    " \n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load(data):\n",
    "    data = np.load(f\"lego_200x200.npz\")\n",
    "\n",
    "    # Training images: [100, 200, 200, 3]\n",
    "    images_train = data[\"images_train\"] / 255.0\n",
    "\n",
    "    # Cameras for the training images \n",
    "    # (camera-to-world transformation matrix): [100, 4, 4]\n",
    "    c2ws_train = data[\"c2ws_train\"]\n",
    "\n",
    "    # Validation images: \n",
    "    images_val = data[\"images_val\"] / 255.0\n",
    "\n",
    "    # Cameras for the validation images: [10, 4, 4]\n",
    "    # (camera-to-world transformation matrix): [10, 200, 200, 3]\n",
    "    c2ws_val = data[\"c2ws_val\"]\n",
    "\n",
    "    # Test cameras for novel-view video rendering: \n",
    "    # (camera-to-world transformation matrix): [60, 4, 4]\n",
    "    c2ws_test = data[\"c2ws_test\"]\n",
    "\n",
    "    # Camera focal length\n",
    "    focal = data[\"focal\"]  # float\n",
    "    return focal, images_train, c2ws_train, images_val, c2ws_val, c2ws_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2.1: Create Rays from Cameras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2.2: Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2.3: Putting the Dataloading All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(c2w, x_c):\n",
    "    if x_c.dim() == 2:\n",
    "        x_c = x_c.unsqueeze(-1)\n",
    "    c2w=torch.tensor(c2w)\n",
    "    xc=torch.cat([x_c, torch.ones_like(x_c[:, :1, :])], dim=1)\n",
    "\n",
    "    c2w=c2w.to(device)\n",
    "    xc=xc.to(device)\n",
    "    x_w=torch.bmm(c2w.float(),xc.float())\n",
    "    \n",
    "    return x_w.squeeze(-1)\n",
    "\n",
    "def transform_check(c2w,x_c):\n",
    "    c2w_inv = torch.inverse(c2w)\n",
    "    return x_c == transform(c2w_inv, transform(c2w, x_c))\n",
    "\n",
    "def pixel_to_camera(K, uv, s):\n",
    "    ox=K[0,2]\n",
    "    oy=K[1,2]\n",
    "    \n",
    "    xc = s * (uv[:,0] - ox)/K[0,0]\n",
    "    yc = s * (uv[:,1] - oy) /K[1,1]\n",
    "\n",
    "    return torch.stack([xc,yc, torch.tensor(s*torch.ones_like(xc))],dim=1)\n",
    "\n",
    "def pixel_to_ray_batch(K, c2w, uv):\n",
    "\n",
    "    c2w=torch.tensor(c2w)\n",
    "    w2c=torch.inverse(c2w)\n",
    "    r=w2c[:,:3,:3]\n",
    "    t=w2c[:,:3,3]\n",
    "\n",
    "    \n",
    "    ray_o= torch.bmm(-torch.inverse(torch.tensor(r)), t.unsqueeze(-1)) \n",
    "    ray_o=ray_o.squeeze(-1)\n",
    "\n",
    "    K=torch.tensor(K)\n",
    "    uv=torch.tensor(uv)\n",
    "\n",
    "    xw=pixel_to_camera(K,uv,1)\n",
    "    xw=transform(c2w,xw).squeeze(-1)\n",
    "    xw=xw[:,:3]\n",
    "\n",
    "    xw=xw.to(device)\n",
    "    ray_o=ray_o.to(device)\n",
    "    temp=xw-ray_o\n",
    "    ray_d= (temp)/torch.norm(temp,dim=1,keepdim=True)\n",
    "    return ray_o, ray_d\n",
    "\n",
    "def pixel_to_ray(K, c2w, uv):\n",
    "    w2c=np.linalg.inv(c2w)\n",
    "    r=w2c[:2,:2]\n",
    "    r1=np.linalg.inv(r)\n",
    "    t=w2c[0,3]\n",
    "    r1=torch.tensor(r1).to(device)\n",
    "    \n",
    "    \n",
    "    ray_o=torch.bmm(-r1,t.unsqueeze(-1)).squeeze(-1)\n",
    "    xw=pixel_to_camera(K,uv,1)\n",
    "    xw=transform(c2w,xw).squeeze(-1)\n",
    "    # print(xw.shape)\n",
    "\n",
    "\n",
    "    ray_d= (xw-ray_o)/torch.norm(xw-ray_o,dim=1)\n",
    "    return ray_o, ray_d\n",
    "\n",
    "\n",
    "def Ray_Sample_im(images, K, c2ws, m_im,n_samples, offset=.5):\n",
    "    # irst sample M images, and then sample N // M rays from every image\n",
    "    m,x,y,z=images.shape\n",
    "    rand_ims = np.random.randint(0, m, m_im)\n",
    "    rays_os = []\n",
    "    rays_ds = []\n",
    "    for i in range(m_im):\n",
    "        im = images[rand_ims[i]]\n",
    "        c2w = c2ws[rand_ims[i]]\n",
    "        H,W = im.shape[:2]\n",
    "        uv = torch.rand(n_samples, 2)\n",
    "        uv[:,0] = (uv[:,0]+x+offset) * W\n",
    "        uv[:,1] = (uv[:,1]+y+offset) * H\n",
    "        uv = uv.long()\n",
    "        rays_o, rays_d = pixel_to_ray(K, c2w, uv)\n",
    "        rays_o = rays_o.to(device)\n",
    "        rays_d = rays_d.to(device)\n",
    "        rays_os.append(rays_o)\n",
    "        rays_ds.append(rays_d)\n",
    "    return torch.cat(rays_os, dim=0), torch.cat(rays_ds, dim=0)\n",
    "        \n",
    "def sample_along_rays(rayo,rayd,near=2, far=6, n_sampels=64, perturb=True):\n",
    "    # t = t + (np.random.rand(t.shape) * t_width)\n",
    "    t = np.linspace(near, far, n_sampels)\n",
    "    t_width= (far-near)/n_sampels\n",
    "    # print(t_width)\n",
    "    # print(t.shape)\n",
    "    if perturb:\n",
    "        ts=t.shape\n",
    "        ts=int(ts[0])\n",
    "        # print(ts)\n",
    "        t = t + (np.random.rand(ts) * t_width)\n",
    "    t = torch.tensor(t).float().to(device)\n",
    "    t=t.expand(rayo.shape[0], n_sampels).unsqueeze(-1)\n",
    "    rayo=torch.tensor(rayo).to(device)\n",
    "    rayd=torch.tensor(rayd).to(device)\n",
    "    return rayo.unsqueeze(1)+ rayd.unsqueeze(1) *t\n",
    "\n",
    "\n",
    "def get_K(im, focal):\n",
    "    h,w,z=im.shape\n",
    "    return torch.tensor(np.array([[focal, 0, w/2], [0, focal, h/2], [0, 0, 1]]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class RaysData(Dataset):\n",
    "    def __init__(self, images,K, c2ws, focal, n_samples=64, near=2, far=6, offset=0.5):\n",
    "        self.images = images\n",
    "        self.c2ws = c2ws\n",
    "        self.focal = focal\n",
    "        self.n_samples = n_samples\n",
    "        self.near = near\n",
    "        self.offset = offset\n",
    "        self.far = far\n",
    "        self.K = get_K(images[0], focal)\n",
    "    def uvs(self):\n",
    "        H,W = self.images.shape[1],self.images.shape[2]\n",
    "        # print(H.type,W.type)  \n",
    "\n",
    "\n",
    "        uv = torch.stack(torch.meshgrid(torch.arange(self.images.shape[0]), torch.arange(H), torch.arange(W)), dim=-1).to(device).float()\n",
    "        uv[..., 0] += self.offset\n",
    "        uv[..., 1] += self.offset\n",
    "        \n",
    "        uv = uv.reshape(-1, 2)\n",
    "\n",
    "\n",
    "        # uv = torch.stack(torch.meshgrid(torch.arange(self.images.shape[0]), torch.arange(H), torch.arange(W)), dim=-1).to(device).float()\n",
    "        # uv[:,1] = (uv[:,1] + self.offset) \n",
    "        # uv[:,2] = (uv[:,2] + self.offset)\n",
    "        # # uv = uv.long()\n",
    "        # print(uv.shape)\n",
    "        # print(uv.type)\n",
    "\n",
    "        return uv\n",
    "    def pixels(self):\n",
    "\n",
    "        H, W = self.images.shape[1], self.images.shape[2]\n",
    "        all_uv = []\n",
    "        for i in range(self.images.shape[0]): \n",
    "            uv = torch.stack(torch.meshgrid(torch.arange(H), torch.arange(W)), dim=-1).to(device).float()\n",
    "            uv[:,: 0] += self.offset\n",
    "            uv[:,:, 1] += self.offset \n",
    "            uv = uv.reshape(-1, 2)\n",
    "            all_uv.append(uv)\n",
    "\n",
    "            uv_int = uv.long()\n",
    "            # im=self.images.numpy()\n",
    "            # uv_int=uv_int.cpu().numpy()\n",
    "            # image_pixels = self.images[i, uv_int[:, 1], uv_int[:, 0]]  \n",
    "            all_uv.append(uv_int)\n",
    "        # all_uv = torch.tensor(all_uv).to(device)\n",
    "        # all_uv =all_uv.cpu()\n",
    "        all_uv = torch.cat(all_uv, dim=0)\n",
    "        \n",
    "        im_pixels = self.images[:, all_uv[:, 1], all_uv[:, 0]]\n",
    "        all_uv =np.array(all_uv)\n",
    "        # print(all_uv.shape)\n",
    "        return im_pixels\n",
    "        # H, W = self.images.shape[1], self.images.shape[2]\n",
    "        # all_uv = []\n",
    "\n",
    "        # for i in range(self.images.shape[0]):\n",
    "        #     uv = torch.stack(torch.meshgrid(torch.arange(H), torch.arange(W)), dim=-1).to(device).float()\n",
    "        #     uv[..., 0] += self.offset\n",
    "        #     uv[..., 1] += self.offset\n",
    "        #     uv = uv.reshape(-1, 2)\n",
    "        #     all_uv.append(uv)\n",
    "        \n",
    "        # return torch.cat(all_uv, dim=0) \n",
    "        # print(self.images.reshape(-1,3).shape)\n",
    "        # pix=self.images.reshape(-1,3)\n",
    "        # print(pix.reshape(-1).shape)\n",
    "        # return self.images.reshape(-1,3)\n",
    "    def rays(self):\n",
    "        uv = self.uvs()\n",
    "        ray_o, ray_d = pixel_to_ray_batch(self.K, self.c2ws, uv)\n",
    "        return ray_o, ray_d\n",
    "\n",
    "    def sample_rays(self, n_samples, offset=.5):\n",
    "\n",
    "        H, W = self.images.shape[1], self.images.shape[2]\n",
    "        all_uv = []\n",
    "\n",
    "        for i in range(self.images.shape[0]):\n",
    "            uv = torch.stack(torch.meshgrid(torch.arange(H), torch.arange(W)), dim=-1).to(device).float()\n",
    "            # print(\"UV\",uv.shape)\n",
    "            uv[:,: 0] += offset\n",
    "            uv[:,:, 1] += offset\n",
    "            uv = uv.reshape(-1, 2)\n",
    "            all_uv.append(uv)\n",
    "        \n",
    "        all_uv = torch.cat(all_uv, dim=0) \n",
    "        # print(\"UVALL\",all_uv.shape)\n",
    "\n",
    "        sampled_indices = torch.randint(0, all_uv.shape[0], (n_samples,))\n",
    "        im_flat=self.images.reshape(-1,3)\n",
    "        sampled_pixels = im_flat[sampled_indices]\n",
    "\n",
    "        sampled_uv = all_uv[sampled_indices]  \n",
    "        sampled_c2ws = self.c2ws[sampled_indices // (H * W)]\n",
    "        # print(sampled_indices.shape)\n",
    "        # print(self.c2ws.shape)\n",
    "        # print(sampled_uv.shape)\n",
    "        ray_o, ray_d = pixel_to_ray_batch(self.K, sampled_c2ws, sampled_uv)\n",
    "\n",
    "        return ray_o, ray_d, torch.tensor(sampled_pixels).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import viser, time  # pip install viser\n",
    "# import numpy as np\n",
    "\n",
    "# focal, images_train, c2ws_train, images_val, c2ws_val, c2ws_test= data_load(\"lego_200x200.npz\")\n",
    "# # images_train=images_train[:10]\n",
    "# # c2ws_train=c2ws_train[:10]\n",
    "# # print(images_train[0].shape)\n",
    "# K=get_K(images_train[0], focal)\n",
    "# dataset = RaysData(images_train, K, c2ws_train,focal)\n",
    "# rays_o, rays_d, pixels = dataset.sample_rays(100) # Should expect (B, 3)\n",
    "# points = sample_along_rays(rays_o, rays_d, perturb=True)\n",
    "# points = points.cpu().numpy()\n",
    "# rays_o = rays_o.cpu().numpy()\n",
    "# K = K.cpu().numpy()\n",
    "# rays_d = rays_d.cpu().numpy()\n",
    "# pixels = pixels.cpu().numpy()\n",
    "# H, W = images_train.shape[1:3]\n",
    "# # ---------------------------------------\n",
    "\n",
    "# # server = viser.ViserServer(share=True)\n",
    "\n",
    "# # for i, (image, c2w) in enumerate(zip(images_train, c2ws_train)):\n",
    "# #     server.add_camera_frustum(\n",
    "# #         f\"/cameras/{i}\",\n",
    "\n",
    "# #         fov=2 * np.arctan2(H / 2, K[0, 0]),\n",
    "#         # aspect=W / H,\n",
    "# #         scale=0.15,\n",
    "# #         wxyz=viser.transforms.SO3.from_matrix(c2w[:3, :3]).wxyz,\n",
    "# #         position=c2w[:3, 3],\n",
    "# #         image=image\n",
    "# #     )\n",
    "# # for i, (o, d) in enumerate(zip(rays_o, rays_d)):\n",
    "# #     server.add_spline_catmull_rom(\n",
    "# #         f\"/rays/{i}\", positions=np.stack((o, o + d * 6.0)),\n",
    "# #     )\n",
    "# # server.add_point_cloud(\n",
    "# #     f\"/samples\",\n",
    "# #     colors=np.zeros_like(points).reshape(-1, 3),\n",
    "# #     points=points.reshape(-1, 3),\n",
    "# #     point_size=0.02,\n",
    "# # )\n",
    "# # time.sleep(1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Visualize Cameras, Rays and Samples\n",
    "# import viser, time\n",
    "# import numpy as np\n",
    "\n",
    "# # --- You Need to Implement These ------\n",
    "# focal, images_train, c2ws_train, images_val, c2ws_val, c2ws_test= data_load(\"lego_200x200.npz\")\n",
    "# K=get_K(images_train[0], focal)\n",
    "\n",
    "# dataset = RaysData(images_train, K, c2ws_train,focal)\n",
    "\n",
    "# # This will check that your uvs aren't flipped\n",
    "# uvs_start = 0\n",
    "# uvs_end = 40_000\n",
    "# sample_uvs = dataset.uvs()[uvs_start:uvs_end] # These are integer coordinates of widths / heights (xy not yx) of all the pixels in an image\n",
    "# # uvs are array of xy coordinates, so we need to index into the 0th image tensor with [0, height, width], so we need to index with uv[:,1] and then uv[:,0]\n",
    "\n",
    "\n",
    "# # sample_uvs = sample_uvs.cpu().numpy().astype(np.int32)\n",
    "# print(sample_uvs.shape)\n",
    "# dataset_pixels = dataset.pixels()\n",
    "# dataset_pixels=dataset_pixels[uvs_start:uvs_end]\n",
    "# print(dataset_pixels.shape) \n",
    "# print(images_train[0, sample_uvs[:,1], sample_uvs[:,0]].shape)\n",
    "# print(images_train[0,dataset_pixels[:,1], dataset_pixels[:,0]])\n",
    "# print(images_train[0, sample_uvs[:,1], sample_uvs[:,0]] )\n",
    "# assert np.all(images_train[0, sample_uvs[:,1], sample_uvs[:,0]] == dataset_pixels)    \n",
    "\n",
    "# # # Uncoment this to display random rays from the first image\n",
    "# indices = np.random.randint(low=0, high=40_000, size=100)\n",
    "\n",
    "# # # Uncomment this to display random rays from the top left corner of the image\n",
    "# # indices_x = np.random.randint(low=100, high=200, size=100)\n",
    "# # indices_y = np.random.randint(low=0, high=100, size=100)\n",
    "# # indices = indices_x + (indices_y * 200)\n",
    "\n",
    "# data = {\"rays_o\": dataset.rays_o[indices], \"rays_d\": dataset.rays_d[indices]}\n",
    "# points = sample_along_rays(data[\"rays_o\"], data[\"rays_d\"], random=True)\n",
    "# # ---------------------------------------\n",
    "\n",
    "# server = viser.ViserServer(share=True)\n",
    "# for i, (image, c2w) in enumerate(zip(images_train, c2ws_train)):\n",
    "#   server.add_camera_frustum(\n",
    "#     f\"/cameras/{i}\",\n",
    "#     fov=2 * np.arctan2(H / 2, K[0, 0]),\n",
    "#     aspect=W / H,\n",
    "#     scale=0.15,\n",
    "#     wxyz=viser.transforms.SO3.from_matrix(c2w[:3, :3]).wxyz,\n",
    "#     position=c2w[:3, 3],\n",
    "#     image=image\n",
    "#   )\n",
    "# for i, (o, d) in enumerate(zip(data[\"rays_o\"], data[\"rays_d\"])):\n",
    "#   positions = np.stack((o, o + d * 6.0))\n",
    "#   server.add_spline_catmull_rom(\n",
    "#       f\"/rays/{i}\", positions=positions,\n",
    "#   )\n",
    "# server.add_point_cloud(\n",
    "#     f\"/samples\",\n",
    "#     colors=np.zeros_like(points).reshape(-1, 3),\n",
    "#     points=points.reshape(-1, 3),\n",
    "#     point_size=0.03,\n",
    "# )\n",
    "# time.sleep(1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def volrend(sigmas, rgbs, step_size=(6.0 - 2.0) / 64):\n",
    "    # x=torch.exp(-sigmas*step_size)\n",
    "    # x=1-x\n",
    "    # T=torch.cumprod(x,dim=2)\n",
    "    # T=T.to(device)\n",
    "    # x=x.to(device)\n",
    "    # rgbs=rgbs.to(device)\n",
    "\n",
    "    # T = torch.cat([torch.ones((T.shape[0], 1, 1), device=device), T[:, :-1]], dim=1)    \n",
    "    # c=T*x*rgbs\n",
    "    # return torch.sum(c,dim=1)\n",
    "    x = 1-torch.exp(-sigmas *step_size)\n",
    "    T = torch.cumprod(1 - x, dim=1) \n",
    "    \n",
    "    T = T.to(device)\n",
    "    x = x.to(device)\n",
    "    rgbs = rgbs.to(device)\n",
    "    T = torch.cat([torch.ones((T.shape[0], 1, 1), device=device), T[:, :-1]], dim=1)\n",
    "    \n",
    "    c = T * x * rgbs  \n",
    "    \n",
    "    ret = torch.sum(c, dim=1)  \n",
    "    \n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import torch\n",
    "# torch.manual_seed(42)\n",
    "# sigmas = torch.rand((10, 64, 1))\n",
    "# rgbs = torch.rand((10, 64, 3))\n",
    "# step_size = (6.0 - 2.0) / 64\n",
    "# rendered_colors = volrend(sigmas, rgbs, step_size)\n",
    "\n",
    "# correct = torch.tensor([\n",
    "#     [0.5006, 0.3728, 0.4728],\n",
    "#     [0.4322, 0.3559, 0.4134],\n",
    "#     [0.4027, 0.4394, 0.4610],\n",
    "#     [0.4514, 0.3829, 0.4196],\n",
    "#     [0.4002, 0.4599, 0.4103],\n",
    "#     [0.4471, 0.4044, 0.4069],\n",
    "#     [0.4285, 0.4072, 0.3777],\n",
    "#     [0.4152, 0.4190, 0.4361],\n",
    "#     [0.4051, 0.3651, 0.3969],\n",
    "#     [0.3253, 0.3587, 0.4215]\n",
    "#   ])\n",
    "# rendered_colors = rendered_colors.cpu()\n",
    "# print(rendered_colors)\n",
    "# correct = correct.cpu()\n",
    "# print(correct.shape)\n",
    "\n",
    "# assert torch.allclose(rendered_colors, correct, rtol=1e-4, atol=1e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=2, L=10, L1=4, device='cuda'):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.L = L\n",
    "        self.L1 = L1\n",
    "        self.device = device\n",
    "\n",
    "        self.l1 = nn.Linear(3 * (2 * L + 1), 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, 256)\n",
    "        self.l4 = nn.Linear(256, 256)\n",
    "        # CONCAT\n",
    "\n",
    "        self.l5 = nn.Linear(256, 256)\n",
    "        self.l6 = nn.Linear(256, 256)\n",
    "        self.l7 = nn.Linear(256, 256)\n",
    "        self.l8 = nn.Linear(256, 256)\n",
    "        # SPLIT OFF\n",
    "\n",
    "        self.l9 = nn.Linear(256, 1)\n",
    "        # DENSITY\n",
    "\n",
    "        self.l10 = nn.Linear(256, 256)\n",
    "        self.l11 = nn.Linear(3 * (2 * L1 + 1)+256, 128)\n",
    "        self.l12 = nn.Linear(128, 3)\n",
    "        # COLOR\n",
    "\n",
    "        self.relu=nn.ReLU()\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x,rd):\n",
    "        x = x.to(self.device)\n",
    "        rd = rd.to(self.device)\n",
    "        xpe=PE(x,self.L)\n",
    "        # print(\"XPE\",xpe.shape)\n",
    "        rdpe=PE(rd,self.L1)\n",
    "        # print(\"RDPE\",rdpe.shape)\n",
    "        xpe=xpe.to(self.device)\n",
    "        rdpe=rdpe.to(self.device)\n",
    "        a = self.relu(self.l1(xpe))\n",
    "        a = self.relu(self.l2(a))\n",
    "        a = self.relu(self.l3(a))\n",
    "        a = self.relu(self.l4(a))\n",
    "        # CONCAT\n",
    "\n",
    "        a = self.relu(self.l5(a))\n",
    "        a = self.relu(self.l6(a))\n",
    "        a = self.relu(self.l7(a))\n",
    "        # a = self.l8(a)\n",
    "        a = self.relu(self.l8(a))\n",
    "        # SPLIT OFF\n",
    "        d= self.relu(self.l9(a))\n",
    "        # DENSITY\n",
    "        a = self.l10(a)\n",
    "        # print(\"A\",a.shape)\n",
    "        # print(\"RDPE\",rdpe.shape)\n",
    "\n",
    "        # A torch.Size([1000, 64, 256])\n",
    "        # RDPE torch.Size([1000, 27])\n",
    "\n",
    "        # a=torch.cat([a,rdpe],dim=-1)    # RuntimeError: Tensors must have same number of dimensions: got 3 and 2\n",
    "        # a=torch.cat([a,rdpe.unsqueeze(1)],dim=-1)\n",
    "        a=torch.cat((a,rdpe),dim=2)\n",
    "        # print(\"RGB\",a.shape)\n",
    "        a = self.relu(self.l11(a))\n",
    "        # print(\"AAAAAAAAAAAAAAAAAAAAAAAAAAAA128\")\n",
    "        a = self.sigmoid(self.l12(a))\n",
    "        # print(\"AAAAAAAAAAAAAAAAAAAAAAAA3\")\n",
    "        return a.float(), d.float()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def PE(x, L):\n",
    "\n",
    "\n",
    "    freqs = 2.0 ** torch.arange(L).float().to(device)\n",
    "    x_input = x.unsqueeze(-1) * freqs * 2 * torch.pi\n",
    "    pe = torch.cat([torch.sin(x_input), torch.cos(x_input)], dim=-1)\n",
    "    \n",
    "    oe = torch.cat([x, pe.reshape(*x.shape[:-1], -1)], dim=-1) \n",
    "    \n",
    "    return oe\n",
    "\n",
    "\n",
    "def compute_psnr(pred, target):\n",
    "    mse = torch.mean((pred - target) ** 2)\n",
    "    if mse == 0:\n",
    "        return 100  \n",
    "    return 10 * torch.log10(1.0 / mse)\n",
    "\n",
    "def rand_sample(im, N):\n",
    "    x, y, z = im.shape\n",
    "    points = []\n",
    "    rand_x = np.random.randint(0, x, N)\n",
    "    rand_y = np.random.randint(0, y, N)\n",
    "    colors = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        points.append([rand_y[i] / y, rand_x[i] / x])  # Normalized coordinates\n",
    "        c = im[rand_x[i], rand_y[i]]\n",
    "        c = c / 255  # Normalize color values to [0, 1]\n",
    "        colors.append(c)\n",
    "    # print(\"points\",points)\n",
    "    return np.array(points), np.array(colors)\n",
    "\n",
    "def create_image(ima, model, L):\n",
    "    h, w, _ = ima.shape\n",
    "    h, w = h // 3, w // 3  \n",
    "    # print(h,w)\n",
    "    x = torch.linspace(0, w-1, w).repeat(h, 1) / w  \n",
    "    # print(x.shape)\n",
    "    y = torch.linspace(0, h-1, h).repeat(w, 1).transpose(0, 1) / h  \n",
    "    # print(y.shape)\n",
    "    # Stack x and y to get (x, y) pairs and reshape them into a single tensor\n",
    "    all_coords = torch.stack([x, y], dim=-1).view(-1, 2)\n",
    "    # print(all_coords.shape)\n",
    "    # print(all_coords)\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predicted_pixels = model(all_coords)\n",
    "    \n",
    "    predicted_pixels_cpu = predicted_pixels.cpu()\n",
    "\n",
    "    predicted_image = predicted_pixels_cpu.reshape(h, w, 3).numpy()\n",
    "\n",
    "    predicted_image = np.clip(predicted_image * 255, 0, 255).astype(np.uint8)\n",
    "    # predicted_image=np.rot90(predicted_image,3)\n",
    "    \n",
    "    return predicted_image\n",
    "\n",
    "\n",
    "def train_mod(images_train, c2ws_train, images_val, c2ws_val,focal, model, N, L, iterations, lr=0.005):\n",
    "    # images_train=images_train.to(device)\n",
    "    # c2ws_train=c2ws_train.to(device)\n",
    "    # images_val=images_val.to(device)\n",
    "    # c2ws_val=c2ws_val.to(device)\n",
    "    \n",
    "    # im = np.array(im)\n",
    "    model.to(device)\n",
    "    loss_func = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "    train_data=RaysData(images_train, get_K(images_train[0], focal), c2ws_train, focal)\n",
    "    val_data=RaysData(images_val, get_K(images_val[0], focal), c2ws_val, focal)\n",
    "\n",
    "\n",
    "    loss_list = []\n",
    "    psnr_list = []\n",
    "    predicted_images = []\n",
    "    model.train()\n",
    "    val_loss =[]\n",
    "    val_psnr=[]\n",
    "    # print(\"TRAIN DATA\",images_train[].shape)\n",
    "    for i in range(iterations):\n",
    "        rayo,rayd, pixels = train_data.sample_rays(N)\n",
    "        # print(rayo.shape)\n",
    "        xw=sample_along_rays(rayo,rayd,perturb=True)\n",
    "        rayd=rayd.unsqueeze(1).expand(-1, xw.shape[1], -1)\n",
    "        # print(\"RAYSD\", rayd.type)\n",
    "        optimizer.zero_grad()\n",
    "        xw = xw.to(device).float()\n",
    "        rayd = rayd.to(device).float()\n",
    "           \n",
    "        # print(\"XWTUPE\",xw.dtype)\n",
    "        # print(rayd.dtype)\n",
    "        # print(\"rad_sahpe\",rayd.shape)\n",
    "\n",
    "        rgb, d = model(xw, rayd)\n",
    "        \n",
    "        predrgb = volrend(d, rgb)\n",
    "        # print(\"PRED RGB\", predrgb.shape)\n",
    "        # print(\"PIXELS\", pixels.shape)\n",
    "        # colors=images_train.reshape(-1,3)\n",
    "        # colors=colors.cpu().numpy()\n",
    "        # colors=colors[pixels]\n",
    "        loss = loss_func(predrgb.float(), pixels.float()).float()\n",
    "        # loss = sum(loss for l.float() in loss.values())\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        psnr = compute_psnr(predrgb, pixels)    \n",
    "        psnr_list.append(psnr)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 20 ==0:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                rayo, rayd, pixels = val_data.sample_rays(1000)\n",
    "                xw = sample_along_rays(rayo, rayd, perturb=False)\n",
    "                xw = xw.to(device).float()\n",
    "                rayd=rayd.unsqueeze(1).expand(-1, xw.shape[1], -1)\n",
    "\n",
    "                rayd = rayd.to(device).float()\n",
    "                rgb, d = model(xw, rayd)\n",
    "                predrgb = volrend(d, rgb)\n",
    "                loss = loss_func(predrgb, pixels)\n",
    "                psnr = compute_psnr(predrgb, pixels)\n",
    "                val_loss.append(loss.item())\n",
    "                val_psnr.append(psnr)\n",
    "                print(f\"Validation Loss: {loss.item()}, PSNR: {psnr}\")\n",
    "                model.train()\n",
    "    return model, loss_list, psnr_list, predicted_images, val_loss, val_psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jae\\AppData\\Local\\Temp\\ipykernel_22508\\1118788951.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ray_o= torch.bmm(-torch.inverse(torch.tensor(r)), t.unsqueeze(-1))\n",
      "C:\\Users\\Jae\\AppData\\Local\\Temp\\ipykernel_22508\\1118788951.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  K=torch.tensor(K)\n",
      "C:\\Users\\Jae\\AppData\\Local\\Temp\\ipykernel_22508\\1118788951.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  uv=torch.tensor(uv)\n",
      "C:\\Users\\Jae\\AppData\\Local\\Temp\\ipykernel_22508\\1118788951.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.stack([xc,yc, torch.tensor(s*torch.ones_like(xc))],dim=1)\n",
      "C:\\Users\\Jae\\AppData\\Local\\Temp\\ipykernel_22508\\1118788951.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  c2w=torch.tensor(c2w)\n",
      "C:\\Users\\Jae\\AppData\\Local\\Temp\\ipykernel_22508\\1118788951.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rayo=torch.tensor(rayo).to(device)\n",
      "C:\\Users\\Jae\\AppData\\Local\\Temp\\ipykernel_22508\\1118788951.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rayd=torch.tensor(rayd).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0944333215028575, PSNR: 10.248747346070335\n",
      "Validation Loss: 0.05872927333862456, PSNR: 12.311453726795685\n",
      "Validation Loss: 0.05740318819301266, PSNR: 12.410639860657732\n",
      "Validation Loss: 0.05974273653474681, PSNR: 12.237148882815937\n",
      "Validation Loss: 0.06189474071016124, PSNR: 12.083462520718665\n",
      "Validation Loss: 0.0653786896173978, PSNR: 11.845637882162862\n",
      "Validation Loss: 0.0647819777176382, PSNR: 11.885457976233702\n",
      "Validation Loss: 0.06556418993203035, PSNR: 11.833333002782387\n",
      "Validation Loss: 0.06517419074684715, PSNR: 11.859243526622908\n",
      "Validation Loss: 0.06793577691136263, PSNR: 11.679014536552172\n",
      "Validation Loss: 0.06022603737191307, PSNR: 12.202157106983622\n",
      "Validation Loss: 0.04860600645081062, PSNR: 13.13310059806425\n",
      "Validation Loss: 0.053113837503427144, PSNR: 12.747923194365466\n",
      "Validation Loss: 0.04316395941979967, PSNR: 13.648787244316436\n",
      "Validation Loss: 0.03995101132993718, PSNR: 13.9847222237122\n",
      "Validation Loss: 0.03662661569357901, PSNR: 14.36203208376292\n",
      "Validation Loss: 0.042578457678813596, PSNR: 13.708100740792819\n",
      "Validation Loss: 0.04198304689186387, PSNR: 13.769260459750253\n",
      "Validation Loss: 0.04039097286836104, PSNR: 13.937156861646882\n",
      "Validation Loss: 0.03716393393643124, PSNR: 14.298783206079895\n",
      "Validation Loss: 0.044530626616516, PSNR: 13.513411936138821\n",
      "Validation Loss: 0.039671132891322514, PSNR: 14.01525397170802\n",
      "Validation Loss: 0.03905728452052284, PSNR: 14.082979545645\n",
      "Validation Loss: 0.04056573825300343, PSNR: 13.918406159196264\n",
      "Validation Loss: 0.039396678679960384, PSNR: 14.045403896405652\n",
      "Validation Loss: 0.04489946612272384, PSNR: 13.477588229461983\n",
      "Validation Loss: 0.03917902545121343, PSNR: 14.069463709554789\n",
      "Validation Loss: 0.03808292304223355, PSNR: 14.192697248780933\n",
      "Validation Loss: 0.04256181263696965, PSNR: 13.709798844378621\n",
      "Validation Loss: 0.04702806325702229, PSNR: 13.276429063083024\n",
      "Validation Loss: 0.042930823776958824, PSNR: 13.672307779926074\n",
      "Validation Loss: 0.03656548370019853, PSNR: 14.369286772120383\n",
      "Validation Loss: 0.04094195474830635, PSNR: 13.878314260570825\n",
      "Validation Loss: 0.037866894888891264, PSNR: 14.217403058617194\n",
      "Validation Loss: 0.04165518039475186, PSNR: 13.803309804048245\n",
      "Validation Loss: 0.047304016321898706, PSNR: 13.251019841588468\n",
      "Validation Loss: 0.043675979519134406, PSNR: 13.597573463339325\n",
      "Validation Loss: 0.04647261551183076, PSNR: 13.32802884431178\n",
      "Validation Loss: 0.048916626469336616, PSNR: 13.105435016821732\n",
      "Validation Loss: 0.045345057078300166, PSNR: 13.434700470959713\n",
      "Validation Loss: 0.04151902274512804, PSNR: 13.817528772548087\n",
      "Validation Loss: 0.044115749253878636, PSNR: 13.554063403959532\n",
      "Validation Loss: 0.03982820558113365, PSNR: 13.998092598414136\n",
      "Validation Loss: 0.043481200075367044, PSNR: 13.616984779501088\n",
      "Validation Loss: 0.04752567922269742, PSNR: 13.230716676121972\n",
      "Validation Loss: 0.04924990304964664, PSNR: 13.075946200911394\n",
      "Validation Loss: 0.04742245041879807, PSNR: 13.240160088591582\n",
      "Validation Loss: 0.049874385477041525, PSNR: 13.021224423943993\n",
      "Validation Loss: 0.04784477804366684, PSNR: 13.20165455809341\n",
      "Validation Loss: 0.05046313912341037, PSNR: 12.97025737140457\n",
      "Validation Loss: 0.0462194019480177, PSNR: 13.351756783367055\n",
      "Validation Loss: 0.04623739468321344, PSNR: 13.35006644885554\n",
      "Validation Loss: 0.04045724592173308, PSNR: 13.930036846325402\n",
      "Validation Loss: 0.04117746985447075, PSNR: 13.853403420793223\n",
      "Validation Loss: 0.05064946511943772, PSNR: 12.95425136619658\n",
      "Validation Loss: 0.04334720624144427, PSNR: 13.630388878768864\n",
      "Validation Loss: 0.04892584033407765, PSNR: 13.104617063072897\n",
      "Validation Loss: 0.04205554757482813, PSNR: 13.761767080177005\n",
      "Validation Loss: 0.04319570415297956, PSNR: 13.645594419680467\n",
      "Validation Loss: 0.04578989350503093, PSNR: 13.392303665232134\n",
      "Validation Loss: 0.044194799299473626, PSNR: 13.5462883400905\n",
      "Validation Loss: 0.04520782387151939, PSNR: 13.447863977066243\n",
      "Validation Loss: 0.04684643693101301, PSNR: 13.293234353001186\n",
      "Validation Loss: 0.047301163046424885, PSNR: 13.251281806483473\n",
      "Validation Loss: 0.04613021319271348, PSNR: 13.360145382769469\n",
      "Validation Loss: 0.043780226369904446, PSNR: 13.587219972005629\n",
      "Validation Loss: 0.044300717337347806, PSNR: 13.535892414263763\n",
      "Validation Loss: 0.04462745980074367, PSNR: 13.50397832539178\n",
      "Validation Loss: 0.04537250100209186, PSNR: 13.43207281093881\n",
      "Validation Loss: 0.049902061725209265, PSNR: 13.018815109420512\n",
      "Validation Loss: 0.04261427303175126, PSNR: 13.704449159035814\n",
      "Validation Loss: 0.04860939028672923, PSNR: 13.132798262978032\n",
      "Validation Loss: 0.048136522872217785, PSNR: 13.175252840640653\n",
      "Validation Loss: 0.038429049291257435, PSNR: 14.15340359555116\n",
      "Validation Loss: 0.04908230549554425, PSNR: 13.090750457723184\n",
      "Validation Loss: 0.05079773438871738, PSNR: 12.941556570944288\n",
      "Validation Loss: 0.046372593323826185, PSNR: 13.337386161004332\n",
      "Validation Loss: 0.04234184145402413, PSNR: 13.732302583635876\n",
      "Validation Loss: 0.04619728691857504, PSNR: 13.35383529008596\n",
      "Validation Loss: 0.052413034542105066, PSNR: 12.805606953491143\n",
      "Validation Loss: 0.037236740856848326, PSNR: 14.29028337621611\n",
      "Validation Loss: 0.0457630445616223, PSNR: 13.394850901519204\n",
      "Validation Loss: 0.045856649644520284, PSNR: 13.385976786520828\n",
      "Validation Loss: 0.0440897274349859, PSNR: 13.556625859967095\n",
      "Validation Loss: 0.04401119590100615, PSNR: 13.564368303429529\n",
      "Validation Loss: 0.04771927887161059, PSNR: 13.213061279616001\n",
      "Validation Loss: 0.048416600890052584, PSNR: 13.150057036698229\n",
      "Validation Loss: 0.045518666600729334, PSNR: 13.418104684149863\n",
      "Validation Loss: 0.04399465013240723, PSNR: 13.566001316782593\n",
      "Validation Loss: 0.042454855192253296, PSNR: 13.720726361001361\n",
      "Validation Loss: 0.04341406925173662, PSNR: 13.623695053212003\n",
      "Validation Loss: 0.04232836603852734, PSNR: 13.733684958573726\n",
      "Validation Loss: 0.04931085144466013, PSNR: 13.070574984924868\n",
      "Validation Loss: 0.04669926918634691, PSNR: 13.306899158111957\n",
      "Validation Loss: 0.04664890198924468, PSNR: 13.311585741173564\n",
      "Validation Loss: 0.04481677003402761, PSNR: 13.48559446483376\n",
      "Validation Loss: 0.05230426552540032, PSNR: 12.814628920389538\n",
      "Validation Loss: 0.04879122534406737, PSNR: 13.116582748681704\n",
      "Validation Loss: 0.0425135864496307, PSNR: 13.714722563694144\n",
      "Validation Loss: 0.04640665851036192, PSNR: 13.334197016252718\n",
      "Validation Loss: 0.04653101287611859, PSNR: 13.32257493743942\n",
      "Validation Loss: 0.050172861972424154, PSNR: 12.995311251356831\n",
      "Validation Loss: 0.03740312143357229, PSNR: 14.270921527511614\n",
      "Validation Loss: 0.046684190362916995, PSNR: 13.308301686974723\n",
      "Validation Loss: 0.0467867418786325, PSNR: 13.29877197026205\n",
      "Validation Loss: 0.04635626129654038, PSNR: 13.338915978110537\n",
      "Validation Loss: 0.042757757308550935, PSNR: 13.689850820896366\n",
      "Validation Loss: 0.04778847142770585, PSNR: 13.206768606934656\n",
      "Validation Loss: 0.04663068269523527, PSNR: 13.313282262224732\n",
      "Validation Loss: 0.04545310130564852, PSNR: 13.42436479131136\n",
      "Validation Loss: 0.0538024095775599, PSNR: 12.691982737238659\n",
      "Validation Loss: 0.057347309288530234, PSNR: 12.414869541978922\n",
      "Validation Loss: 0.04982807281993479, PSNR: 13.02525909534807\n",
      "Validation Loss: 0.04407234446776971, PSNR: 13.558338461923015\n",
      "Validation Loss: 0.048860722941595866, PSNR: 13.11040111508651\n",
      "Validation Loss: 0.049928377893597616, PSNR: 13.016525433641405\n",
      "Validation Loss: 0.04967331511174997, PSNR: 13.03876854969622\n",
      "Validation Loss: 0.046880079111670886, PSNR: 13.29011664103167\n",
      "Validation Loss: 0.04595097652311016, PSNR: 13.377052548090603\n",
      "Validation Loss: 0.04508281259428393, PSNR: 13.459889973270469\n",
      "Validation Loss: 0.048283386911348994, PSNR: 13.162022732615462\n",
      "Validation Loss: 0.05429050126460032, PSNR: 12.652761484816402\n"
     ]
    }
   ],
   "source": [
    "focal, images_train, c2ws_train, images_val, c2ws_val, c2ws_test= data_load(\"lego_200x200.npz\")\n",
    "model=MLP(input_size=3,L=10,L1=4,device='cuda')\n",
    "modela, loss_list, psnr_list, predicted_images, val_loss, val_psnr = train_mod(images_train, c2ws_train, images_val, c2ws_val,focal, model, 1000, 20, 20000, lr=0.005)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvision2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
